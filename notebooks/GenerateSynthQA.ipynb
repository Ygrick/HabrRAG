{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "91a90463",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from datasets import load_dataset\n",
    "from datasets.dataset_dict import DatasetDict, IterableDatasetDict\n",
    "from datasets.arrow_dataset import Dataset\n",
    "from datasets.iterable_dataset import IterableDataset\n",
    "from typing import Union\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "85d62a8b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['id', 'language', 'url', 'title', 'text_markdown', 'text_html', 'author', 'original_author', 'original_url', 'lead_html', 'lead_markdown', 'type', 'time_published', 'statistics', 'labels', 'hubs', 'flows', 'tags', 'reading_time', 'format', 'complexity', 'comments'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset = load_dataset('IlyaGusev/habr', split=\"train[:1000]\")\n",
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "386a8c29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_cleaner(text: str) -> str:\n",
    "    \"\"\"Очистка текста перед разбиением на чанки.\"\"\"\n",
    "    text = re.sub(r'<[^>]+>', '', text)\n",
    "    text = re.sub(r'(\\r\\n|\\r|\\n){2,}', r'\\n', text)\n",
    "    text = re.sub(r'[ \\t]+', ' ', text)\n",
    "    return text.strip()\n",
    "\n",
    "\n",
    "def chunk_documents(documents: Union[DatasetDict, Dataset, IterableDatasetDict, IterableDataset], chunk_size: int = 1000, chunk_overlap: int = 100) -> list[Document]:\n",
    "    \"\"\"Разбивает документы на чанки.\"\"\"\n",
    "    print(f\"Разбиваем {len(documents)} документов на чанки (размер: {chunk_size}, перекрытие: {chunk_overlap})\")\n",
    "\n",
    "    text_splitter = RecursiveCharacterTextSplitter(chunk_size=chunk_size, chunk_overlap=chunk_overlap)\n",
    "    chunked_documents: list[Document] = []\n",
    "    global_chunk_id = 0\n",
    "\n",
    "    for doc_data in tqdm(documents):\n",
    "        # Извлекаем текст и метаданные из документа\n",
    "        text = doc_data.get('text_markdown', '')\n",
    "        langchain_doc = Document(page_content=text_cleaner(text))\n",
    "        chunks = text_splitter.split_documents([langchain_doc])\n",
    "\n",
    "        for j, chunk in enumerate(chunks):\n",
    "            # Добавляем метаданные в каждый чанк\n",
    "            chunk.metadata[\"id\"] = doc_data['id']  # ID документа из датасета\n",
    "            chunk.metadata[\"author\"] = doc_data.get('author', 'Unknown')  # Автор\n",
    "            chunk.metadata[\"url\"] = doc_data.get('url', '')  # Ссылка на статью\n",
    "            chunk.metadata[\"title\"] = doc_data.get('title', '')  # Заголовок статьи\n",
    "            chunk.metadata[\"document_chunk_id\"] = j  # ID чанка внутри документа\n",
    "            chunk.metadata[\"global_chunk_id\"] = global_chunk_id  # Глобальный ID чанка\n",
    "            global_chunk_id += 1\n",
    "            chunked_documents.append(chunk)\n",
    "\n",
    "    print(f\"Создано {len(chunked_documents)} чанков\")\n",
    "    return chunked_documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2569327f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Разбиваем 1000 документов на чанки (размер: 1000, перекрытие: 100)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:00<00:00, 1437.54it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Создано 8836 чанков\n",
      "Length of chunked_docs: 8836\n",
      "First chunk: page_content='Абитуриенты, поступающие в Университет Чикаго, уже начиная с этой осени, будут обязаны, помимо сдачи основных экзаменов, представить четырехстраничные презентации в формате PowerPoint — на свободную тему.\n",
      "Частично новые требования связаны с тем, что PowerPoint становится одним из бизнес-инструментов, а кроме того, считают авторы инициативы, в презентации абитуриент сможет раскрыть свои новые качества, которые не смогут проявиться на традиционных экзаменах.\n",
      "Формат PowerPoint стал своего рода языком делового общения. Ежедневно, по оценкам Microsoft, в мире показывается порядка 30 млн презентаций.\n",
      "via AP' metadata={'id': 12730, 'author': 'Юлия Благовещенская (skazala)', 'url': 'https://habr.com/ru/post/12730/', 'title': 'Хочешь в университет — сделай презентацию', 'document_chunk_id': 0, 'global_chunk_id': 0}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "chunked_docs = chunk_documents(dataset)\n",
    "print(f\"Length of chunked_docs: {len(chunked_docs)}\")\n",
    "print(f\"First chunk: {chunked_docs[0]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "797b456f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.messages import SystemMessage, HumanMessage\n",
    "from pydantic import BaseModel, Field\n",
    "import os\n",
    "import time\n",
    "from dotenv import load_dotenv\n",
    "from pathlib import Path\n",
    "import json\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b4f53c2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class QA(BaseModel):\n",
    "    reasoning: str = Field(description=\"Твои рассуждения о том, почему выбран именно этот вопрос\")\n",
    "    question: str = Field(description=\"Текст вопроса\")\n",
    "    answer: str = Field(description=\"Текст ответа\")\n",
    "\n",
    "llm = ChatOpenAI(\n",
    "    model=os.getenv(\"MODEL\"),\n",
    "    api_key=os.getenv(\"KEY\"),\n",
    "    base_url=os.getenv(\"URL\"),\n",
    ")\n",
    "llm_qa = llm.with_structured_output(QA)\n",
    "\n",
    "second_llm = ChatMistralAI(\n",
    "    model=\"mistral-small-2506\",\n",
    "    api_key=os.getenv(\"MISTRAL_API_KEY\"),\n",
    "    timeout=30,\n",
    "    max_tokens=4096,\n",
    "    temperature=0.3,\n",
    ")\n",
    "second_llm_qa = second_llm.with_structured_output(QA)\n",
    "\n",
    "QA_GEN_SYSTEM_PROMPT = \"\"\"\n",
    "# Role\n",
    "Ты — эксперт по созданию обучающих данных для тестирования поисковых систем (RAG). Твоя задача — на основе предоставленного фрагмента текста (чанка) сгенерировать пару: \"Вопрос пользователя\" и \"Идеальный ответ\".\n",
    "\n",
    "# Task\n",
    "Проанализируй предоставленный текст (Context Chunk). Выдели ключевую информацию, которую пользователь мог бы искать. Сформулируй вопрос так, чтобы для ответа на него требовалась эта информация.\n",
    "\n",
    "# Rules for the QUESTION (Строгие правила):\n",
    "1. **Самодостаточность:** Вопрос должен быть понятен БЕЗ контекста. Не используй фразы \"в этом тексте\", \"в данном документе\", \"описанный выше\". Представь, что пользователь ничего не читал, а просто задает вопрос в Google или чат-боту.\n",
    "   - ПЛОХО: \"Какие преимущества описаны?\"\n",
    "   - ХОРОШО: \"Какие преимущества дает использование Kubernetes в банковской сфере?\"\n",
    "2. **Конкретика:** Избегай общих вопросов типа \"О чем текст?\". Вопрос должен касаться конкретной детали, процедуры, условия или определения.\n",
    "3. **Сложность:** Избегай вопросов, на которые можно ответить \"Да\" или \"Нет\".\n",
    "\n",
    "# Rules for the ANSWER (Строгие правила):\n",
    "1. Ответ должен быть точным и основан ТОЛЬКО на предоставленном тексте.\n",
    "2. Ответ должен быть полным предложением, а не обрывком фразы.\n",
    "3. Не начинай ответ с фразы \"В тексте сказано...\". Просто дай информацию.\n",
    "\n",
    "# Generation Strategy (Chain of Thought):\n",
    "Сначала подумай (Reasoning):\n",
    "1. Какие уникальные сущности (имена, названия, термины) есть в тексте?\n",
    "2. Какую проблему решает этот текст?\n",
    "3. Сформулируй вопрос, который содержит эти сущности.\n",
    "\n",
    "# Output Format\n",
    "Верни результат СТРОГО в формате JSON:\n",
    "{\n",
    "  \"reasoning\": \"Твои рассуждения о том, почему выбран именно этот вопрос\",\n",
    "  \"question\": \"Текст вопроса\",\n",
    "  \"answer\": \"Текст ответа\"\n",
    "}\n",
    "\n",
    "Если текст не содержит полезной фактической информации (например, это оглавление или \"вода\"), верни в JSON поле \"question\": null.\n",
    "\n",
    "### Examples:\n",
    "\n",
    "Context: \"Для возврата товара заполните форму А-12 и отправьте на почту support@shop.com в течение 14 дней.\"\n",
    "\n",
    "BAD Output:\n",
    "Q: Как вернуть товар?\n",
    "A: Форма А-12.\n",
    "\n",
    "GOOD Output:\n",
    "Q: Какова процедура оформления возврата товара и в какие сроки это нужно сделать?\n",
    "A: Чтобы оформить возврат, необходимо заполнить форму А-12 и отправить её на электронный адрес support@shop.com. Это нужно сделать не позднее 14 дней с момента покупки.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b06de459",
   "metadata": {},
   "outputs": [],
   "source": [
    "def call_llm_qa(query: str) -> QA:\n",
    "    \"\"\"Вызывает LLM у одного из провайдеров\"\"\"\n",
    "    try:\n",
    "        response: QA = llm_qa.invoke([\n",
    "            SystemMessage(content=QA_GEN_SYSTEM_PROMPT),\n",
    "            HumanMessage(content=f\"Chunk: {query}\")\n",
    "        ])\n",
    "    except:\n",
    "        response: QA = second_llm_qa.invoke([\n",
    "            SystemMessage(content=QA_GEN_SYSTEM_PROMPT),\n",
    "            HumanMessage(content=f\"Chunk: {query}\")\n",
    "        ])\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91eb44a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Какие преимущества дает внедрение MailEssentials на почтовый шлюз предприятия?\n",
      "Внедрение MailEssentials обеспечивает быструю установку без лишней настройки, защищает от спама и фишинга на уровне почтового шлюза, позволяет отказаться от дорогостоящего развертывания и поддержки защиты на рабочих станциях, не требуется обучать пользователей борьбе со спамом и регулярно настраивать правила фильтрации, а также предотвращает накопление почтового мусора в дисковой подсистеме сервера электронной почты.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'id': 304276,\n",
       "  'author': 'ETitovich',\n",
       "  'url': 'https://habr.com/ru/post/304276/',\n",
       "  'title': 'GFI MailEssentials: почта под защитой',\n",
       "  'document_chunk_id': 1,\n",
       "  'global_chunk_id': 2000,\n",
       "  'question': 'Какие преимущества дает внедрение MailEssentials на почтовый шлюз предприятия?',\n",
       "  'answer': 'Внедрение MailEssentials обеспечивает быструю установку без лишней настройки, защищает от спама и фишинга на уровне почтового шлюза, позволяет отказаться от дорогостоящего развертывания и поддержки защиты на рабочих станциях, не требуется обучать пользователей борьбе со спамом и регулярно настраивать правила фильтрации, а также предотвращает накопление почтового мусора в дисковой подсистеме сервера электронной почты.'}]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_dataset = []\n",
    "for sample in chunked_docs[2000:3000:5]:\n",
    "    # Создаём словарь для хранения результатов\n",
    "    temp_qa = {\n",
    "        \"id\": sample.metadata.get(\"id\", \"\"),\n",
    "        \"author\": sample.metadata.get(\"author\", \"\"),\n",
    "        \"url\": sample.metadata.get(\"url\", \"\"),\n",
    "        \"title\": sample.metadata.get(\"title\", \"\"),\n",
    "        \"document_chunk_id\": sample.metadata.get(\"document_chunk_id\", \"\"),\n",
    "        \"global_chunk_id\": sample.metadata.get(\"global_chunk_id\", \"\"),\n",
    "        \"question\": None,\n",
    "        \"answer\": None\n",
    "    }\n",
    "\n",
    "    # Вызываем LLM с предусмотрением того, что она может упасть по лимитам или ограничениям\n",
    "    try:\n",
    "        response: QA = call_llm_qa(sample.page_content)\n",
    "    except:\n",
    "        try:\n",
    "            time.sleep(5)\n",
    "            response: QA = call_llm_qa(sample.page_content)\n",
    "        except:\n",
    "            continue\n",
    "    \n",
    "    # если вопрос или ответ слишком короткие, то скорее всего в чем-то проблема\n",
    "    # и лучше пропустить такой пример\n",
    "    if len(response.question) < 15 or len(response.answer) < 15:\n",
    "        continue\n",
    "    \n",
    "    # Обрабатываем результаты\n",
    "    print(response.question)\n",
    "    print(response.answer)\n",
    "    temp_qa[\"question\"] = response.question\n",
    "    temp_qa[\"answer\"] = response.answer\n",
    "    qa_dataset.append(temp_qa)\n",
    "    \n",
    "    # Сохраняем результаты в файл на каждой итерации, чтоб не потерять прогресс\n",
    "    with open(\"qa_dataset.jsonl\", \"a\", encoding=\"utf-8\") as f:\n",
    "        f.write(json.dumps(temp_qa, ensure_ascii=False) + \"\\n\")\n",
    "    \n",
    "    time.sleep(3)\n",
    "\n",
    "qa_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d21278a3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
